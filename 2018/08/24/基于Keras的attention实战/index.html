<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/NextLegend.github.io/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/NextLegend.github.io/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/NextLegend.github.io/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/NextLegend.github.io/atom.xml" title="Next Legend!" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/NextLegend.github.io/favicon.ico?v=5.1.0" />






<meta name="description" content="该教程是基于Kears的Attention实战">
<meta name="keywords" content="Love Promise">
<meta property="og:type" content="article">
<meta property="og:title" content="基于Kears的Attention实战">
<meta property="og:url" content="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/基于Keras的attention实战/index.html">
<meta property="og:site_name" content="Next Legend!">
<meta property="og:description" content="该教程是基于Kears的Attention实战">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://img-blog.csdn.net/20180821160600590?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:image" content="https://img-blog.csdn.net/20180821160943644?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">
<meta property="og:updated_time" content="2018-08-24T03:34:13.116Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于Kears的Attention实战">
<meta name="twitter:description" content="该教程是基于Kears的Attention实战">
<meta name="twitter:image" content="https://img-blog.csdn.net/20180821160600590?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/NextLegend.github.io/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/基于Keras的attention实战/"/>





  <title> 基于Kears的Attention实战 | Next Legend! </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f2e5400f17b3cf03a0b6c0fc85e89d57";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/NextLegend.github.io/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Next Legend!</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一天进步一点点</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/NextLegend.github.io/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/NextLegend.github.io/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/NextLegend.github.io/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/NextLegend.github.io/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-linux">
          <a href="/NextLegend.github.io/Linux" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Linux
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/NextLegend.github.io/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/NextLegend.github.io/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/NextLegend.github.io/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/NextLegend.github.io/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      
        
        <li class="menu-item menu-item-github">
          <a href="/NextLegend.github.io/github" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-github"></i> <br />
            
            github
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://legendtianjin.github.io/NextLegend.github.io/NextLegend.github.io/2018/08/24/基于Keras的attention实战/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="赵小亮">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/NextLegend.github.io/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Next Legend!">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                基于Kears的Attention实战
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-08-24T11:34:48+08:00">
                2018-08-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/NextLegend.github.io/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/NextLegend.github.io/2018/08/24/基于Keras的attention实战/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/08/24/基于Keras的attention实战/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          
              <div class="post-description">
                  该教程是基于Kears的Attention实战
              </div>
          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>要点：<br>该教程为基于Kears的Attention实战，环境配置：<br>Wn10+CPU i7-6700<br>Pycharm 2018<br>python 3.6<br>numpy 1.14.5<br>Keras 2.0.2<br>Matplotlib 2.2.2<br><strong><em>强调：各种库的版本型号一定要配置对，因为Keras以及Tensorflow升级更新比较频繁，很多函数更新后要么更换了名字，要么没有这个函数了，所以大家务必重视。</em></strong><br><strong>相关代码我放在了我的代码仓库里哈，欢迎大家下载，这里附上地址：<a href="https://download.csdn.net/download/jinyuan7708/10617858" target="_blank" rel="noopener">基于Kears的Attention实战</a></strong><br>笔者信息：Next_Legend QQ:1219154092 人工智能 自然语言处理 图像处理 神经网络<br>——2018.8.21于天津大学</p>
<hr>
<p>一、导读<br>最近两年，尤其在今年，注意力机制(Attention)及其变种Attention逐渐热了起来，在很多顶会Paper中都或多或少的用到了attention,所以小编出于好奇，整理了这篇基于Kears的Attention实战，本教程仅从代码的角度来看Attention。通过一个简单的例子，探索Attention机制是如何在模型中起到特征选择作用的。<br>二、代码实战（一）<br>1、导入相关库文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> attention_utils <span class="keyword">import</span> get_activations, get_data</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, merge</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure></p>
<p>2、数据生成函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span><span class="params">(n, input_dim, attention_column=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Data generation. x is purely random except that it's first value equals the target y.</span></span><br><span class="line"><span class="string">    In practice, the network should learn that the target = x[attention_column].</span></span><br><span class="line"><span class="string">    Therefore, most of its attention should be focused on the value addressed by attention_column.</span></span><br><span class="line"><span class="string">    :param n: the number of samples to retrieve.</span></span><br><span class="line"><span class="string">    :param input_dim: the number of dimensions of each element in the series.</span></span><br><span class="line"><span class="string">    :param attention_column: the column linked to the target. Everything else is purely random.</span></span><br><span class="line"><span class="string">    :return: x: model inputs, y: model targets</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x = np.random.standard_normal(size=(n, input_dim))</span><br><span class="line">    y = np.random.randint(low=<span class="number">0</span>, high=<span class="number">2</span>, size=(n, <span class="number">1</span>))</span><br><span class="line">    x[:, attention_column] = y[:, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>
<p>3、模型定义函数<br>将输入进行一次变换后，计算出Attention权重，将输入乘上Attention权重，获得新的特征。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">()</span>:</span></span><br><span class="line">    inputs = Input(shape=(input_dim,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># ATTENTION PART STARTS HERE</span></span><br><span class="line">    attention_probs = Dense(input_dim, activation=<span class="string">'softmax'</span>, name=<span class="string">'attention_vec'</span>)(inputs)</span><br><span class="line">    attention_mul =merge([inputs, attention_probs], output_shape=<span class="number">32</span>, name=<span class="string">'attention_mul'</span>, mode=<span class="string">'mul'</span>)</span><br><span class="line">    <span class="comment"># ATTENTION PART FINISHES HERE</span></span><br><span class="line"></span><br><span class="line">    attention_mul = Dense(<span class="number">64</span>)(attention_mul)</span><br><span class="line">    output = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(attention_mul)</span><br><span class="line">    model = Model(input=[inputs], output=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>4、主函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    N = <span class="number">10000</span></span><br><span class="line">    inputs_1, outputs = get_data(N, input_dim)</span><br><span class="line"></span><br><span class="line">    m = build_model()</span><br><span class="line">    m.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">    print(m.summary())</span><br><span class="line"></span><br><span class="line">    m.fit([inputs_1], outputs, epochs=<span class="number">20</span>, batch_size=<span class="number">64</span>, validation_split=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">    testing_inputs_1, testing_outputs = get_data(<span class="number">1</span>, input_dim)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Attention vector corresponds to the second matrix.</span></span><br><span class="line">    <span class="comment"># The first one is the Inputs output.</span></span><br><span class="line">    attention_vector = get_activations(m, testing_inputs_1,</span><br><span class="line">                                       print_shape_only=<span class="keyword">True</span>,</span><br><span class="line">                                       layer_name=<span class="string">'attention_vec'</span>)[<span class="number">0</span>].flatten()</span><br><span class="line">    print(<span class="string">'attention ='</span>, attention_vector)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot part.</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">    pd.DataFrame(attention_vector, columns=[<span class="string">'attention (%)'</span>]).plot(kind=<span class="string">'bar'</span>,</span><br><span class="line">                                                                   title=<span class="string">'Attention Mechanism as '</span></span><br><span class="line">                                                                         <span class="string">'a function of input'</span></span><br><span class="line">                                                                         <span class="string">' dimensions.'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p>
<p>5、运行结果<br>代码中，attention_column为1，也就是说，label只与数据的第1个特征相关。从运行结果中可以看出，Attention权重成功地获取了这个信息。<br><img src="https://img-blog.csdn.net/20180821160600590?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p>
<hr>
<p>三、代码实战（二）<br>1、导入相关库文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> merge</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.layers.recurrent <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> attention_utils <span class="keyword">import</span> get_activations, get_data_recurrent</span><br><span class="line">INPUT_DIM = <span class="number">2</span></span><br><span class="line">TIME_STEPS = <span class="number">20</span></span><br><span class="line"><span class="comment"># if True, the attention vector is shared across the input_dimensions where the attention is applied.</span></span><br><span class="line">SINGLE_ATTENTION_VECTOR = <span class="keyword">False</span></span><br><span class="line">APPLY_ATTENTION_BEFORE_LSTM = <span class="keyword">False</span></span><br></pre></td></tr></table></figure></p>
<p>2、数据生成函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_3d_block</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="comment"># inputs.shape = (batch_size, time_steps, input_dim)</span></span><br><span class="line">    input_dim = int(inputs.shape[<span class="number">2</span>])</span><br><span class="line">    a = Permute((<span class="number">2</span>, <span class="number">1</span>))(inputs)</span><br><span class="line">    a = Reshape((input_dim, TIME_STEPS))(a) <span class="comment"># this line is not useful. It's just to know which dimension is what.</span></span><br><span class="line">    a = Dense(TIME_STEPS, activation=<span class="string">'softmax'</span>)(a)</span><br><span class="line">    <span class="keyword">if</span> SINGLE_ATTENTION_VECTOR:</span><br><span class="line">        a = Lambda(<span class="keyword">lambda</span> x: K.mean(x, axis=<span class="number">1</span>), name=<span class="string">'dim_reduction'</span>)(a)</span><br><span class="line">        a = RepeatVector(input_dim)(a)</span><br><span class="line">    a_probs = Permute((<span class="number">2</span>, <span class="number">1</span>), name=<span class="string">'attention_vec'</span>)(a)</span><br><span class="line">    output_attention_mul = merge([inputs, a_probs], name=<span class="string">'attention_mul'</span>, mode=<span class="string">'mul'</span>)</span><br><span class="line">    <span class="keyword">return</span> output_attention_mul</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_attention_applied_after_lstm</span><span class="params">()</span>:</span></span><br><span class="line">    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))</span><br><span class="line">    lstm_units = <span class="number">32</span></span><br><span class="line">    lstm_out = LSTM(lstm_units, return_sequences=<span class="keyword">True</span>)(inputs)</span><br><span class="line">    attention_mul = attention_3d_block(lstm_out)</span><br><span class="line">    attention_mul = Flatten()(attention_mul)</span><br><span class="line">    output = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(attention_mul)</span><br><span class="line">    model = Model(input=[inputs], output=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_attention_applied_before_lstm</span><span class="params">()</span>:</span></span><br><span class="line">    inputs = Input(shape=(TIME_STEPS, INPUT_DIM,))</span><br><span class="line">    attention_mul = attention_3d_block(inputs)</span><br><span class="line">    lstm_units = <span class="number">32</span></span><br><span class="line">    attention_mul = LSTM(lstm_units, return_sequences=<span class="keyword">False</span>)(attention_mul)</span><br><span class="line">    output = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(attention_mul)</span><br><span class="line">    model = Model(input=[inputs], output=output)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>3、主函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">   N = <span class="number">300000</span></span><br><span class="line">   <span class="comment"># N = 300 -&gt; too few = no training</span></span><br><span class="line">   inputs_1, outputs = get_data_recurrent(N, TIME_STEPS, INPUT_DIM)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> APPLY_ATTENTION_BEFORE_LSTM:</span><br><span class="line">       m = model_attention_applied_before_lstm()</span><br><span class="line">   <span class="keyword">else</span>:</span><br><span class="line">       m = model_attention_applied_after_lstm()</span><br><span class="line"></span><br><span class="line">   m.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">   print(m.summary())</span><br><span class="line"></span><br><span class="line">   m.fit([inputs_1], outputs, epochs=<span class="number">1</span>, batch_size=<span class="number">64</span>, validation_split=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">   attention_vectors = []</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">       testing_inputs_1, testing_outputs = get_data_recurrent(<span class="number">1</span>, TIME_STEPS, INPUT_DIM)</span><br><span class="line">       attention_vector = np.mean(get_activations(m,</span><br><span class="line">                                                  testing_inputs_1,</span><br><span class="line">                                                  print_shape_only=<span class="keyword">True</span>,</span><br><span class="line">                                                  layer_name=<span class="string">'attention_vec'</span>)[<span class="number">0</span>], axis=<span class="number">2</span>).squeeze()</span><br><span class="line">       print(<span class="string">'attention ='</span>, attention_vector)</span><br><span class="line">       <span class="keyword">assert</span> (np.sum(attention_vector) - <span class="number">1.0</span>) &lt; <span class="number">1e-5</span></span><br><span class="line">       attention_vectors.append(attention_vector)</span><br><span class="line"></span><br><span class="line">   attention_vector_final = np.mean(np.array(attention_vectors), axis=<span class="number">0</span>)</span><br><span class="line">   <span class="comment"># plot part.</span></span><br><span class="line">   <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">   <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">   pd.DataFrame(attention_vector_final, columns=[<span class="string">'attention (%)'</span>]).plot(kind=<span class="string">'bar'</span>,</span><br><span class="line">                                                                        title=<span class="string">'Attention Mechanism as '</span></span><br><span class="line">                                                                              <span class="string">'a function of input'</span></span><br><span class="line">                                                                              <span class="string">' dimensions.'</span>)</span><br><span class="line">   plt.show()</span><br></pre></td></tr></table></figure></p>
<p>4、运行结果<br>代码中，attention_column为10，11，也就是说，label只与数据的第10，11个特征相关。从运行结果中可以看出，Attention权重成功地获取了这个信息。<br><img src="https://img-blog.csdn.net/20180821160943644?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppbnl1YW43NzA4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p>
<p>##<strong>相关代码放在代码仓库里哈，欢迎大家下载，这里附上地址：<a href="https://download.csdn.net/download/jinyuan7708/10617858" target="_blank" rel="noopener">基于Kears的Attention实战</a></strong></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/NextLegend.github.io/2018/08/24/new-Types/" rel="next" title="new Types">
                <i class="fa fa-chevron-left"></i> new Types
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/NextLegend.github.io/2018/08/24/基于python+opencv的图像目标区域自动提取（本项目为提取纸张中的内容）/" rel="prev" title="基于python+opencv的图像目标区域自动提取">
                基于python+opencv的图像目标区域自动提取 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2018/08/24/基于Keras的attention实战/"
     data-title="基于Kears的Attention实战"
     data-content=""
     data-url="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/基于Keras的attention实战/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2018/08/24/基于Keras的attention实战/"
           data-title="基于Kears的Attention实战" data-url="https://legendtianjin.github.io/NextLegend.github.io/2018/08/24/基于Keras的attention实战/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/NextLegend.github.io/uploads/avatar.jpg"
               alt="赵小亮" />
          <p class="site-author-name" itemprop="name">赵小亮</p>
           
              <p class="site-description motion-element" itemprop="description">记录生活点点滴滴</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/NextLegend.github.io/archives/">
                <span class="site-state-item-count">9</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/NextLegend.github.io/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/NextLegend.github.io/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/NextLegend.github.io/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/LegendTianjin || github" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:yourname@gmail.com || envelope" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  E-Mail
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://plus.google.com/yourname || google" target="_blank" title="Google">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Google
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/yourname || twitter" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://youtube.com/yourname || youtube" target="_blank" title="YouTube">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  YouTube
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="skype:yourname?call|chat || skype" target="_blank" title="Skype">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  Skype
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">赵小亮</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  <script type="text/javascript" src="/NextLegend.github.io/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/NextLegend.github.io/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/NextLegend.github.io/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/NextLegend.github.io/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/NextLegend.github.io/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/NextLegend.github.io/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/NextLegend.github.io/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/NextLegend.github.io/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/NextLegend.github.io/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/NextLegend.github.io/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/NextLegend.github.io/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/NextLegend.github.io/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/NextLegend.github.io/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"imwillxue"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
      
      <script src="/NextLegend.github.io/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
      <script src="/NextLegend.github.io/js/src/hook-duoshuo.js?v=5.1.0"></script>
    
    
    <script src="/NextLegend.github.io/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/NextLegend.github.io/js/src/hook-duoshuo.js"></script>
  
















  





  

  

  

  

</body>
</html>
